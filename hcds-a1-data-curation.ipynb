{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Centered Data Science: Assignment A1\n",
    "### University of Washington, Fall 2017 \n",
    "#### October 19, 2017 \n",
    "Alyssa Goodrich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "The goal of this notebook is to construct, analyze, and publish a dataset of monthly traffic on English Wikipedia from January 1 2008 through September 30 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Acquisition\n",
    "Our first step is to access the data from the API. In this case we must use two separate end points because one includes recent data and the other includes legacy data. \n",
    "\n",
    "NOTE: There is an inconsistency in the two data sets. The legacy \"pagecounts\" data includes scrapers/crawlers whereas the pageviews data includes only user generated data.\n",
    "\n",
    "Please see the read me file for more information on the API, endpoints and license information.\n",
    "\n",
    "The goal of this step is to access the data from the API and save it to JSON files that we can later use to extract the data we need and put it in a format we can analyze.\n",
    "\n",
    "** The first step is to access data from the pageviews endpoint and save it to JSON files. There will be one file for each of desktop, mobile-app and mobile-web access points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This code will access data from wikimedia api (referenced in endpoint below), and save it to a JSON file\n",
    "#This code was adapted from code written by Jonathan Morgan. It was accessed 10/13/2017 at the site: http://hcds.yuvi.in/user/alyssacolony/notebooks/data-512-a1/hcds-a1-data-curation.ipynb\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "endpoint = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/aggregate/{project}/{access}/{agent}/{granularity}/{start}/{end}'\n",
    "\n",
    "headers={'User-Agent' : 'https://github.com/alyssacolony', 'From' : 'alcgood@uw.edu'}\n",
    "\n",
    "# Choose start and end date. Data available beginning July 1, 2016. Earlier data available from page counts API 'https://wikimedia.org/api/rest_v1/#!/Pagecounts_data_(legacy)/get_metrics_legacy_pagecounts_aggregate_project_access_site_granularity_start_end'\n",
    "# Data should be formatted: YYYYMMDDHH so July 1, 2016 is formatted as: 2016070100   \n",
    "end_date = '2017100100' # First day of current month\n",
    "start_date = '2015070100' # Start date is when data is available\n",
    "\n",
    "# Set a variable that determines which access type we want to collect data for\n",
    "#Options are: 'desktop', 'mobile-app','mobile-web'\n",
    "access = ['desktop', 'mobile-app','mobile-web'] \n",
    "# Set a variable that determines which agent type we want to collect data for \n",
    "#options are 'user', 'spider', 'all-agents'\n",
    "agent = ['user'] \n",
    "\n",
    "# Create a parameter list for each access type\n",
    "for i in range(len(access)):\n",
    "    params = {'project' : 'en.wikipedia.org',\n",
    "            'access' : 'desktop',\n",
    "            'agent' : 'user', #We choose only user\n",
    "            'granularity' : 'monthly',\n",
    "            'start' : start_date,\n",
    "            'end' : end_date            }\n",
    "\n",
    "#Defines a function that gets data based on specified parameters and saves to a file\n",
    "#Filename structured as \"UserType_AccessPoint_StartDate_EndDate.txt\"\n",
    "def getData():\n",
    "    #This function \n",
    "    for j in range(len(agent)):\n",
    "        thisAgent = agent[j]\n",
    "        for i in range(len(access)):\n",
    "            params['agent'] = thisAgent\n",
    "            params['access'] = access[i]\n",
    "            api_call = requests.get(endpoint.format(**params))\n",
    "            response = api_call.json()\n",
    "            filename = 'pageViews'+ '_'+ params['access']+'_'+start_date[0:6]+'_'+end_date[0:6]+'.json'\n",
    "            with open(filename, 'w') as outfile:\n",
    "                json.dump(response, outfile)\n",
    "            \n",
    "# Run funciton to collect data\n",
    "getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The next step is to access data from the pagecounts endpoint and save it to JSON files. There will be one file for each of desktop-site and mobile-site access points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This code will access data from wikimedia api (referenced in endpoint below), and save it to a JSON file\n",
    "#This code was adapted from code written by Jonathan Morgan. It was accessed 10/13/2017 at the site: http://hcds.yuvi.in/user/alyssacolony/notebooks/data-512-a1/hcds-a1-data-curation.ipynb\n",
    "\n",
    "endpoint = 'https://wikimedia.org/api/rest_v1/metrics/legacy/pagecounts/aggregate/{project}/{access-site}/{granularity}/{start}/{end}'\n",
    "headers={'User-Agent' : 'https://github.com/alyssacolony', 'From' : 'alcgood@uw.edu'}\n",
    "\n",
    "# Choose start and end date. Data available beginning July 1, 2016. Earlier data available from page counts API 'https://wikimedia.org/api/rest_v1/#!/Pagecounts_data_(legacy)/get_metrics_legacy_pagecounts_aggregate_project_access_site_granularity_start_end'\n",
    "# Data should be formatted: YYYYMMDDHH so July 1, 2016 is formatted as: 2016070100   \n",
    "end_date = '2016073100' # Last date this data is available\n",
    "start_date = '2008010100' # Start date is earliest date data is available\n",
    "\n",
    "# Set a variable that determines which access type we want to collect data for\n",
    "#Options are: 'desktop-site', 'mobile-site'\n",
    "access = ['desktop-site', 'mobile-site'] \n",
    "\n",
    "# Create a parameter list for each access type\n",
    "for i in range(len(access)):\n",
    "    params = {'project' : 'en.wikipedia.org',\n",
    "            'access-site' : access[i],\n",
    "            'granularity' : 'monthly',\n",
    "            'start' : start_date,\n",
    "            'end' : end_date            }\n",
    "\n",
    "#Defines a function that gets data based on specified parameters and saves to a file\n",
    "#Filename structured as \"UserType_AccessPoint_StartDate_EndDate.txt\"\n",
    "def getData():\n",
    "    #This function \n",
    "    for i in range(len(access)):\n",
    "        params['access-site'] = access[i]\n",
    "        api_call = requests.get(endpoint.format(**params))\n",
    "        response = api_call.json()\n",
    "        filename = 'PageCounts'+ '_'+ params['access-site']+'_'+start_date[0:6]+'_'+end_date[0:6]+'.json'\n",
    "        with open(filename, 'w') as outfile:\n",
    "            json.dump(response, outfile)\n",
    "            \n",
    "# Run funciton to collect data\n",
    "getData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data processing\n",
    "In this step we will process the raw data to put it in a format that we can analyze.  \n",
    "\n",
    "Key steps to processing include:  \n",
    "1) Extract the data we need from the JSON files and put it into a combind dictonary  \n",
    "2) Format and name all necessary data fields to comply with required naming conventions as well combine necessary fields, particularly mobile-app and mobile-web to get a total mobile views number for pageviews data.  \n",
    "3) Write a CSV file with all required fields including: date, year, month, date, pagecount_all_views, pagecount_desktop_views, pagecount_mobile_views, pageview_all_views, pageview_desktop_views, pageview_mobile_views\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "from datetime import datetime\n",
    "\n",
    "# Get list of files to extract from\n",
    "files =  glob.glob('*.json')\n",
    "files\n",
    "\n",
    "\n",
    "# Step 1: The below code block combimes our JSON Files into a single dictionary\n",
    "\n",
    "# NOTE TO OLIVER: I had originally chosen names that would specify \"all agent\" or \"user only\" in an attempt to consider users \n",
    "# who are not familiar with the implications of page view vs page count API. (I later discovered the required naming conventions\n",
    "# and adjusted the names in a later code block. That is why there are extra steps\n",
    "\n",
    "All_PageViews = defaultdict(dict)\n",
    "\n",
    "for file in files:\n",
    "    input_file=open(file, 'r')\n",
    "    json_decode=json.load(input_file)\n",
    "    for item in json_decode['items']:\n",
    "        if item.get('agent') is not None and item.get('access') is not None:\n",
    "            name = str(item.get('agent')) + \"_\"+ str(item.get('access'))\n",
    "        if str(item.get('access-site')) == 'desktop-site':\n",
    "            name = \"all-agents_desktop\"\n",
    "        if str(item.get('access-site')) == 'mobile-site':\n",
    "            name = \"all-agents_mobile\"\n",
    "        if item.get('views') is not None:\n",
    "            All_PageViews[item.get('timestamp')][name] = item.get('views')\n",
    "        if item.get('count') is not None:\n",
    "            All_PageViews[item.get('timestamp')][name] = item.get('count')\n",
    "\n",
    "\n",
    "# Step 2\n",
    "# Create and rename fields to ensure we have all necessary fields for CSV \n",
    "for item in All_PageViews:\n",
    "    #Create a field for all mobile views from pageview API\n",
    "    if (All_PageViews[item].get('user_mobile-app') is not None) and (All_PageViews[item].get('user_mobile-web') is not None):\n",
    "            All_PageViews[item]['pageview_mobile_views'] = All_PageViews[item]['user_mobile-app']+All_PageViews[item]['user_mobile-web']\n",
    "    # rename \"user_desktop field to comply with naming conventions\n",
    "    if (All_PageViews[item].get('user_desktop') is not None):\n",
    "        All_PageViews[item]['pageview_desktop_views'] = All_PageViews[item].pop('user_desktop')\n",
    "    # Create field for all page views from page view API source     \n",
    "    if All_PageViews[item].get('pageview_desktop_views') is not None and All_PageViews[item].get('pageview_mobile_views') is not None:\n",
    "        All_PageViews[item]['pageview_all_views'] = All_PageViews[item]['pageview_desktop_views'] + All_PageViews[item]['pageview_mobile_views'] \n",
    "    # rename \"all-agents_desktop field to comply with naming conventions\n",
    "    if (All_PageViews[item].get('all-agents_desktop') is not None):\n",
    "        All_PageViews[item]['pagecount_desktop_views'] = All_PageViews[item].pop('all-agents_desktop')\n",
    "    # rename \"all-agents_mobile and desktop fields to comply with naming conventions\n",
    "    if (All_PageViews[item].get('all-agents_mobile') is not None):\n",
    "        All_PageViews[item]['pagecount_mobile_views'] = All_PageViews[item].pop('all-agents_mobile')\n",
    "    # Create field for all page views from pagecount API Source\n",
    "    if All_PageViews[item].get('pagecount_desktop_views') is not None and All_PageViews[item].get('pagecount_mobile_views') is None:\n",
    "        All_PageViews[item]['pagecount_all_views'] = All_PageViews[item]['pagecount_desktop_views']  \n",
    "    if All_PageViews[item].get('pagecount_desktop_views') is not None and All_PageViews[item].get('pagecount_mobile_views') is not None:\n",
    "        All_PageViews[item]['pagecount_all_views'] = All_PageViews[item]['pagecount_desktop_views'] + All_PageViews[item]['pagecount_mobile_views'] \n",
    "    #Create Month and Year fields\n",
    "    All_PageViews[item]['Month'] = str(item[4:6])\n",
    "    All_PageViews[item]['Year'] = str(item[0:4])\n",
    "    All_PageViews[item]['Date'] = datetime.strptime(str(item), '%Y%m%d%H').strftime('%m/%Y')\n",
    "\n",
    "Sorted_PageViews = sorted(All_PageViews.items(), key=operator.itemgetter(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3: Write CSV with all necessary data and headers\n",
    "import csv\n",
    "\n",
    "# define headers\n",
    "headers = ['Date',\n",
    "           'Month',\n",
    "           'Year',\n",
    "           'pagecount_all_views',\n",
    "           'pagecount_desktop_views',\n",
    "           'pagecount_mobile_views',\n",
    "           'pageview_all_views',\n",
    "           'pageview_desktop_views',\n",
    "           'pageview_mobile_views',\n",
    "               ]\n",
    "\n",
    "# Open file and initalize writer\n",
    "file = open('en-wikipedia_traffic_200801-201709.csv', 'w',  newline='') \n",
    "writer = csv.writer(file)\n",
    "writer.writerow(headers)\n",
    "\n",
    "# Create a dates variable to create a table ordered by dates\n",
    "dates = []\n",
    "for item in Sorted_PageViews:\n",
    "    dates.append(item[0])\n",
    "\n",
    "# Extract necessary data and write to CSV\n",
    "for date in dates:\n",
    "    line = []\n",
    "    for header in headers:\n",
    "        if All_PageViews[date].get(header) is None or All_PageViews[date][header] == 0:\n",
    "            line.append(None)\n",
    "        else:\n",
    "            line.append(All_PageViews[date][header])\n",
    "    writer.writerow(line)\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analysis\n",
    "\n",
    "\n",
    "In this step we analyze the data that we have extracted with the goal of creating a visualization that shows page view trends on the English language wikipedia site including legacy page count data that includes all views, and current page view data that allows us to consider only user views and exclude spiders from the count.\n",
    "\n",
    "Because of the ridiculously long time I spent on the above steps, I am electing to do the analysis in google graphs. (Obviously I am very new at this. But I am definitely getting my money's worth from this program). \n",
    "\n",
    "**Key steps to google charts analysis:**  \n",
    "1) Copy data from CSV into new sheet at: https://docs.google.com/spreadsheets/u/0/  \n",
    "2) Select \"Insert Chart\" and choose line chart  \n",
    "3) Specifiy each series to be viewed, and specify data for the x axis   \n",
    "4) Adjust formatting and titles  \n",
    "5) Publish to web  \n",
    "\n",
    "![title](pageviewsOnEnglishWikipedia.png)\n",
    "\n",
    "The chart can be viewed here:\n",
    "https://docs.google.com/spreadsheets/d/e/2PACX-1vQaZBhXDCLDwUUGy-9XC7n1uqQ7y0QQ9-yK9m6Oy-CPRRkUd9Ano_VAM0TJrCtLDcorqwh-AESNV5R2/pubchart?oid=463731903&format=image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
